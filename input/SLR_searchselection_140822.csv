category;Source;2001to2021;;not_redundant;Publication Year;Name;Scope;;agnostic/specific;ant/post hoc;Usage;Methodology;Methodologyabr;Objective;Objectiveabr;Pb;Pbabr;comment1;Input;Output;scan;Language;Code.tool;cited_190622_googlescholar;Author;Title;Abstract Note;Publication Title;DOI;Url;Date;Date Added;Date Modified;Pages;ISSN;Key;Item Type;Num Pages;Issue;Volume;Journal Abbreviation;Archive;Extra;Notes;Automatic Tags
Journal paper;SCOPUS;1;search;1;2008;explainVis;Local;Local decomposition;model-agnostic ;post hoc;Post-hoc;Features oriented;F.O.;Local explanation;L.E.;Multiclass;M.C.;cont+discretization;Mixte;Visual graphs;1;R;R;286;"Robnik-Šikonja, M.; Kononenko, I.";Explaining classifications for individual instances;We present a method for explaining predictions for individual instances. The presented approach is general and can be used with all classification models that output probabilities. It is based on the decomposition of a model's predictions on individual contributions of each attribute. Our method works for the so-called black box models such as support vector machines, neural networks, and nearest neighbor algorithms, as well as for ensemble methods such as boosting and random forests. We demonstrate that the generated explanations closely follow the learned models and present a visualization technique that shows the utility of our approach and enables the comparison of different prediction methods. © 2007 IEEE.;IEEE Transactions on Knowledge and Data Engineering;10.1109/TKDE.2007.190734;https://www.scopus.com/inward/record.uri?eid=2-s2.0-52949101606&doi=10.1109%2fTKDE.2007.190734&partnerID=40&md5=86ce78e66c0edd23df4fd5bfbf32a2a3;2008;17/02/2022 13:40;17/02/2022 13:40;589-600;;4D6C2TID;journalArticle;;5;20;;Scopus;;"<p>Cited By :126</p>; <p>Export Date: 17 February 2022</p>";"Classification; Decision support; Decision support systems; Decision visualization; Forecasting; Image retrieval; Information analysis; Information systems; Information visualization; Knowledge modeling; Machine learning; Mathematical models; Nearest eighbor; Neural nets; Neural networks; Prediction models; Robot learning; Support vector machines; Visualization"
Journal paper;SCOPUS;1;search;1;2012;CRF;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Local explanation;G.O. + L.E.;Multiclass;M.C.;;Mixte;Text/statistics;1;Not mentioned;Not mentioned;33;"Liu, S.; Patel, R.Y.; Daga, P.R.; Liu, H.; Fu, G.; Doerksen, R.J.; Chen, Y.; Wilkins, D.E.";Combined rule extraction and feature elimination in supervised classification;There are a vast number of biology related research problems involving a combination of multiple sources of data to achieve a better understanding of the underlying problems. It is important to select and interpret the most important information from these sources. Thus it will be beneficial to have a good algorithm to simultaneously extract rules and select features for better interpretation of the predictive model. We propose an efficient algorithm, Combined Rule Extraction and Feature Elimination (CRF), based on 1-norm regularized random forests. CRF simultaneously extracts a small number of rules generated by random forests and selects important features. We applied CRF to several drug activity prediction and microarray data sets. CRF is capable of producing performance comparable with state-of-the-art prediction algorithms using a small number of decision rules. Some of the decision rules are biologically significant. © 2002-2011 IEEE.;IEEE Transactions on Nanobioscience;10.1109/TNB.2012.2213264;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866521380&doi=10.1109%2fTNB.2012.2213264&partnerID=40&md5=e6e905c03921e2722b73e4f7db8f07d6;2012;17/02/2022 13:39;17/02/2022 13:39;228-236;;J9JFKQZY;journalArticle;;3;11;;Scopus;;"<p>Cited By :19</p>; <p>Export Date: 17 February 2022</p>";"algorithm; Algorithms; article; artificial intelligence; Artificial Intelligence; biology; cannabinoid receptor; Computational Biology; Databases, Factual; Decision rules; decision tree; Decision trees; Decision Trees; DNA microarray; factual database; Feature extraction; genetics; human; Humans; methodology; Microarray data sets; Models, Theoretical; Multi-class classification; multidrug resistance protein; Multiple source; neoplasm; Neoplasms; Oligonucleotide Array Sequence Analysis; P-Glycoprotein; Prediction algorithms; Predictive models; Random forests; Receptors, Cannabinoid; reproducibility; Reproducibility of Results; Research problems; Rule extraction; Supervised classification; theoretical model"
Journal paper;SCOPUS;1;search;1;2012;joha12;Hybrid;Tree and rules ;model-agnostic ;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Mixte;Mixte;1;Not mentioned;Not mentioned;5;"Johansson, U.; Sönströd, C.; Löfström, T.; Boström, H.";Obtaining accurate and comprehensible classifiers using oracle coaching;While ensemble classifiers often reach high levels of predictive performance, the resulting models are opaque and hence do not allow direct interpretation. When employing methods that do generate transparent models, predictive performance typically has to be sacrificed. This paper presents a method of improving predictive performance of transparent models in the very common situation where instances to be classified, i.e., the production data, are known at the time of model building. This approach, named oracle coaching, employs a strong classifier, called an oracle, to guide the generation of a weaker, but transparent model. This is accomplished by using the oracle to predict class labels for the production data, and then applying the weaker method on this data, possibly in conjunction with the original training set. Evaluation on 30 data sets from the UCI repository shows that oracle coaching significantly improves predictive performance, measured by both accuracy and area under ROC curve, compared to using training data only. This result is shown to be robust for a variety of methods for generating the oracles and transparent models. More specifically, random forests and bagged radial basis function networks are used as oracles, while J48 and JRip are used for generating transparent models. The evaluation further shows that significantly better results are obtained when using the oracle-classified production data together with the original training data, instead of using only oracle data. An analysis of the fidelity of the transparent models to the oracles shows that performance gains can be expected from increasing oracle performance rather than from increasing fidelity. Finally, it is shown that further performance gains can be achieved by adjusting the relative weights of training data and oracle data. © 2012 - IOS Press and the authors. All rights reserved.;Intelligent Data Analysis;10.3233/IDA-2012-0522;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858221652&doi=10.3233%2fIDA-2012-0522&partnerID=40&md5=367b4205fa432da08a28a1ce89118a7a;2012;17/02/2022 13:48;17/02/2022 13:48;247-263;;D2MZL934;journalArticle;;2;16;;Scopus;;"<p>Cited By :4</p>; <p>Export Date: 17 February 2022</p>";"Area under roc curve (AUC); Class labels; Classification (of information); comprehensibility; Data sets; Decision lists; Decision trees; Decision trees (DTs); Ensemble classifiers; oracle coaching; Performance Gain; Predictive performance; Production data; Radial basis function networks; Random forests; Relative weights; Training data; Training sets; UCI repository"
Journal paper;SCOPUS;1;search;1;2014;webb14;Global;;model-specific;ant hoc;Intrinsic to the model ;Features oriented;F.O.;Global overview;G.O.;Binaryclass;B.C;;Categorical;Mixte;1;Java/KNIME;Others;31;"Webb, S.J.; Hanser, T.; Howlin, B.; Krause, P.; Vessey, J.D.";Feature combination networks for the interpretation of statistical machine learning models: Application to Ames mutagenicity;"Background: A new algorithm has been developed to enable the interpretation of black box models. The developed algorithm is agnostic to learning algorithm and open to all structural based descriptors such as fragments, keys and hashed fingerprints. The algorithm has provided meaningful interpretation of Ames mutagenicity predictions from both random forest and support vector machine models built on a variety of structural fingerprints.A fragmentation algorithm is utilised to investigate the model's behaviour on specific substructures present in the query. An output is formulated summarising causes of activation and deactivation. The algorithm is able to identify multiple causes of activation or deactivation in addition to identifying localised deactivations where the prediction for the query is active overall. No loss in performance is seen as there is no change in the prediction; the interpretation is produced directly on the model's behaviour for the specific query. Results: Models have been built using multiple learning algorithms including support vector machine and random forest. The models were built on public Ames mutagenicity data and a variety of fingerprint descriptors were used. These models produced a good performance in both internal and external validation with accuracies around 82%. The models were used to evaluate the interpretation algorithm. Interpretation was revealed that links closely with understood mechanisms for Ames mutagenicity. Conclusion: This methodology allows for a greater utilisation of the predictions made by black box models and can expedite further study based on the output for a (quantitative) structure activity model. Additionally the algorithm could be utilised for chemical dataset investigation and knowledge extraction/human SAR development. © 2014 Webb et al.; licensee Chemistry Central Ltd.";Journal of Cheminformatics;10.1186/1758-2946-6-8;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899072790&doi=10.1186%2f1758-2946-6-8&partnerID=40&md5=d2ade019df343b838b784af1149fe74e;2014;17/02/2022 13:40;17/02/2022 13:40;;;KJ6TUIAX;journalArticle;;1;6;;Scopus;Publisher: Gas Turbine Society of Japan;"<p>Cited By :26</p>; <p>Export Date: 17 February 2022</p>";
Preprint;ArXiv;1;search;1;2014;rfFC;Hybrid;Local decomposition;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview + Local explanation;G.O. + L.E.;Multiclass;M.C.;;Mixte;Mixte;1;R;R;121;"Palczewska, Anna; Palczewski, Jan; Robinson, Richard Marchese; Neagu, Daniel";Interpreting random forest classification models using a feature contribution method;Model interpretation is one of the key aspects of the model evaluation process. The explanation of the relationship between model variables and outputs is relatively easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical significance. For “black box” models, such as random forest, this information is hidden inside the model structure. This work presents an approach for computing feature contributions for random forest classification models. It allows for the determination of the influence of each variable on the model prediction for an individual instance. By analysing feature contributions for a training dataset, the most significant variables can be determined and their typical contribution towards predictions made for individual classes, i.e., class-specific feature contribution “patterns”, are discovered. These patterns represent a standard behaviour of the model and allow for an additional assessment of the model reliability for new data. Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology. The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models.;Advances in Intelligent Systems and Computing;10.1007/978-3-319-04717-1_9;;2014;18/02/2022 10:44;18/02/2022 10:44;193-218;;Z2G23SFR;journalArticle;;;263;;;Publisher: Springer Verlag;;
Journal paper;SCOPUS;1;search;1;2014;liu14;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Regression;Reg.;;Mixte;Text/statistics;1;Not mentioned;Not mentioned;28;"Liu, S.; Dissanayake, S.; Patel, S.; Dang, X.; Mlsna, T.; Chen, Y.; Wilkins, D.";Learning accurate and interpretable models based on regularized random forests regression;"Background: Many biology related research works combine data from multiple sources in an effort to understand the underlying problems. It is important to find and interpret the most important information from these sources. Thus it will be beneficial to have an effective algorithm that can simultaneously extract decision rules and select critical features for good interpretation while preserving the prediction performance. Methods: In this study, we focus on regression problems for biological data where target outcomes are continuous. In general, models constructed from linear regression approaches are relatively easy to interpret. However, many practical biological applications are nonlinear in essence where we can hardly find a direct linear relationship between input and output. Nonlinear regression techniques can reveal nonlinear relationship of data, but are generally hard for human to interpret. We propose a rule based regression algorithm that uses 1-norm regularized random forests. The proposed approach simultaneously extracts a small number of rules from generated random forests and eliminates unimportant features. Results: We tested the approach on some biological data sets. The proposed approach is able to construct a significantly smaller set of regression rules using a subset of attributes while achieving prediction performance comparable to that of random forests regression. Conclusion: It demonstrates high potential in aiding prediction and interpretation of nonlinear relationships of the subject being studied. © 2014 Liu et al.; licensee BioMed Central Ltd.";BMC Systems Biology;10.1186/1752-0509-8-S3-S5;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932130909&doi=10.1186%2f1752-0509-8-S3-S5&partnerID=40&md5=f098b5988118bd9b932e1ebd4aa2add4;2014;17/02/2022 13:39;17/02/2022 13:39;;;KRN3M7VZ;journalArticle;;3;8;;Scopus;Publisher: BioMed Central Ltd.;"<p>Cited By :9</p>; <p>Export Date: 17 February 2022</p>";"algorithm; Algorithms; artificial intelligence; Artificial Intelligence; biology; Computational Biology; Linear Models; procedures; statistical model"
Preprint;ArXiv;1;search;1;2014;SOM;Hybrid;;model-specific;post hoc;Post-hoc;Sample similarity-based;S.S;Pattern discovery;P.D.;Multiclass;M.C.;;Continuous;Visual graphs;1;Not mentioned;Not mentioned;12;"P?o?ski, Piotr; Zaremba, Krzysztof";Visualizing random forest with self-organising map;Random Forest (RF) is a powerful ensemble method for classification and regression tasks. It consists of decision trees set. Although, a single tree is well interpretable for human, the ensemble of trees is a black-box model. The popular technique to look inside the RF model is to visualize a RF proximity matrix obtained on data samples with Multidimensional Scaling (MDS) method. Herein, we present a novel method based on Self-Organising Maps (SOM) for revealing intrinsic relationships in data that lay inside the RF used for classification tasks. We propose an algorithm to learn the SOM with the proximity matrix obtained from the RF. The visualization of RF proximity matrix with MDS and SOM is compared. What is more, the SOM learned with the RF proximity matrix has better classification accuracy in comparison to SOM learned with Euclidean distance. Presented approach enables better understanding of the RF and additionally improves accuracy of the SOM. © 2014 Springer International Publishing.;Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics);10.1007/978-3-319-07176-3_6;;2014;18/02/2022 10:44;18/02/2022 10:44;63-71;9.78E+12;EU9VPQHX;journalArticle;;PART 2;8468 LNAI;;;Publisher: Springer Verlag;;
Journal paper;SCOPUS;1;search;1;2015;ALPA;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Mixte;Text/statistics;1;Weka package/Java;Others;67;"Junque De Fortuny, E.; Martens, D.";Active Learning-Based Pedagogical Rule Extraction;Many of the state-of-the-art data mining techniques introduce nonlinearities in their models to cope with complex data relationships effectively. Although such techniques are consistently included among the top classification techniques in terms of predictive power, their lack of transparency renders them useless in any domain where comprehensibility is of importance. Rule-extraction algorithms remedy this by distilling comprehensible rule sets from complex models that explain how the classifications are made. This paper considers a new rule extraction technique, based on active learning. The technique generates artificial data points around training data with low confidence in the output score, after which these are labeled by the black-box model. The main novelty of the proposed method is that it uses a pedagogical approach without making any architectural assumptions of the underlying model. It can therefore be applied to any black-box technique. Furthermore, it can generate any rule format, depending on the chosen underlying rule induction technique. In a large-scale empirical study, we demonstrate the validity of our technique to extract trees and rules from artificial neural networks, support vector machines, and random forests, on 25 data sets of varying size and dimensionality. Our results show that not only do the generated rules explain the black-box models well (thereby facilitating the acceptance of such models), the proposed algorithm also performs significantly better than traditional rule induction techniques in terms of accuracy as well as fidelity. © 2012 IEEE.;IEEE Transactions on Neural Networks and Learning Systems;10.1109/TNNLS.2015.2389037;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027923673&doi=10.1109%2fTNNLS.2015.2389037&partnerID=40&md5=3488312b82a6293aad3b2dbcc6775fb4;2015;17/02/2022 13:45;17/02/2022 13:45;2664-2677;;7VS9I9JS;journalArticle;;11;26;;Scopus;Publisher: Institute of Electrical and Electronics Engineers Inc.;"<p>Cited By :40</p>; <p>Export Date: 17 February 2022</p>";"Active Learning; Artificial intelligence; Classification technique; Complex networks; comprehensibility; Data mining; Decision trees; Empirical studies; Extraction; Neural networks; Pedagogical approach; Random forests; Rule extraction; Rule extraction algorithms; Support vector machines"
Preprint;ArXiv;1;search;1;2015;LOFB-DRF;Global;;model-specific;post hoc;Post-hoc;Size reduction;S.R.;Global overview;G.O.;Multiclass;M.C.;;Mixte;Text/statistics;1;Java/WEKA;Others;10;"Fawagreh, Khaled; Gaber, Mohamad Medhat; Elyan, Eyad";An Outlier Detection-based Tree Selection Approach to Extreme Pruning of Random Forests;Random Forest (RF) is an ensemble classification technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance in terms of predictive accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofolds. First, it investigates how an unsupervised learning technique, namely, Local Outlier Factor (LOF) can be used to identify diverse trees in the RF. Second, trees with the highest LOF scores are then used to produce an extension of RF termed LOFB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, but mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 10 real datasets prove the superiority of our proposed extension over the traditional RF. Unprecedented pruning levels reaching 99% have been achieved at the time of boosting the predictive accuracy of the ensemble. The notably high pruning level makes the technique a good candidate for real-time applications.;ArXiv;;http://arxiv.org/abs/1503.05187;2015-03;18/02/2022 10:44;18/02/2022 10:44;;;8C3H5FNX;journalArticle;;;;;;;;
Journal paper;SCOPUS;1;search;1;2015;paul15;Global;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview;G.O.;Multiclass;M.C.;;Mixte;Text/statistics;1;Not mentioned;Not mentioned;16;"Paul, J.; Dupont, P.";Inferring statistically significant features from random forests;Embedded feature selection can be performed by analyzing the variables used in a Random Forest. Such a multivariate selection takes into account the interactions between variables but is not straightforward to interpret in a statistical sense. We propose a statistical procedure to measure variable importance that tests if variables are significantly useful in combination with others in a forest. We show experimentally that this new importance index correctly identifies relevant variables. The top of the variable ranking is largely correlated with Breiman[U+05F3]s importance index based on a permutation test. Our measure has the additional benefit to produce p-values from the forest voting process. Such p-values offer a very natural way to decide which features are significantly relevant while controlling the false discovery rate. Practical experiments are conducted on synthetic and real data including low and high-dimensional datasets for binary or multi-class problems. Results show that the proposed technique is effective and outperforms recent alternatives by reducing the computational complexity of the selection process by an order of magnitude while keeping similar performances. © 2014 Elsevier B.V..;Neurocomputing;10.1016/j.neucom.2014.07.067;https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922653973&doi=10.1016%2fj.neucom.2014.07.067&partnerID=40&md5=e2b08f31c6eb68da0795e474b674bfbb;2015;17/02/2022 13:42;17/02/2022 13:42;471-480;;F7PYFGWZ;journalArticle;;PB;150;;Scopus;Publisher: Elsevier B.V.;"<p>Cited By :8</p>; <p>Export Date: 17 February 2022</p>";"Article; Clustering algorithms; controlled study; data analysis; data processing; Decision trees; Embedded feature selections; False discovery rate; Feature extraction; High dimensional datasets; High-dimensional data analysis; information processing; random forest; Random forests; selection bias; Significance test; statistical significance; statistics; Synthetic and real data; Tree ensembles; Variable importances"
Preprint;ArXiv;1;search;1;2016;LIME;Local;;model-agnostic ;post hoc;Post-hoc;Features oriented + Sample similarity-based;F.O. + S.S.;Local explanation;L.E.;Multiclass;M.C.;cont+discretization;Mixte;Mixte;1;Python;Python;9164;"Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos";"""why should i trust you?"" explaining the predictions of any classifier";Despite widespread adoption in NLP, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. In this work, we describe LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner. We further present a method to explain models by presenting representative individual predictions and their explanations in a non-redundant manner. We propose a demonstration of these ideas on different NLP tasks such as document classification, politeness detection, and sentiment analysis, with classifiers like neural networks and SVMs. The user interactions include explanations of free-form text, challenging users to identify the better classifier from a pair, and perform basic feature engineering to improve the classifiers.;NAACL-HLT 2016 - 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Demonstrations Session;10.18653/v1/n16-3020;;2016;18/02/2022 10:44;18/02/2022 10:44;97-101;;LHUZ3GZ7;journalArticle;;;;;;Publisher: Association for Computational Linguistics (ACL);;
Journal paper;SCOPUS;1;search;1;2017;SVC;Global;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview;G.O.;Regression;Reg.;;Mixte;Visual graphs;1;Python;Python;35;"Hur, J.-H.; Ihm, S.-Y.; Park, Y.-H.";A variable impacts measurement in random forest for mobile cloud computing;Recently, the importance of mobile cloud computing has increased. Mobile devices can collect personal data from various sensors within a shorter period of time and sensor-based data consists of valuable information from users. Advanced computation power and data analysis technology based on cloud computing provide an opportunity to classify massive sensor data into given labels. Random forest algorithm is known as black box model which is hardly able to interpret the hidden process inside. In this paper, we propose a method that analyzes the variable impact in random forest algorithm to clarify which variable affects classification accuracy the most. We apply Shapley Value with random forest to analyze the variable impact. Under the assumption that every variable cooperates as players in the cooperative game situation, Shapley Value fairly distributes the payoff of variables. Our proposed method calculates the relative contributions of the variables within its classification process. In this paper, we analyze the influence of variables and list the priority of variables that affect classification accuracy result. Our proposed method proves its suitability for data interpretation in black box model like a random forest so that the algorithm is applicable in mobile cloud computing environment. © 2017 Jae-Hee Hur et al.;Wireless Communications and Mobile Computing;10.1155/2017/6817627;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029767711&doi=10.1155%2f2017%2f6817627&partnerID=40&md5=743e274472aa48a56fda8754f4185c13;2017;17/02/2022 13:39;17/02/2022 13:39;;;F5W4R8B2;journalArticle;;;2017;;Scopus;Publisher: Hindawi Limited;"<p>Cited By :21</p>; <p>Export Date: 17 February 2022</p>";"Classification accuracy; Classification process; Cloud computing; Computation power; Cooperative game; Data interpretation; Decision trees; Game theory; Mobile cloud computing; Network function virtualization; Random forest algorithm; Relative contribution; Sensor based data"
Preprint;ArXiv;1;search;1;2017;Modelextraction;Hybrid;;model-agnostic ;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Continuous;Mixte;1;Not mentioned;Not mentioned;132;"Bastani, Osbert; Kim, Carolyn; Bastani, Hamsa";Interpreting Blackbox Models via Model Extraction;Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model---as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overfitting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.;ArXiv;;http://arxiv.org/abs/1705.08504;2017-05;18/02/2022 10:44;18/02/2022 10:44;;;W4TBBSR3;journalArticle;;;;;;;;
Journal paper;SCOPUS;1;search;1;2017;IPM;Global;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview;G.O.;Multiclass;M.C.;;Mixte;Visual graphs;1;R;R;31;Epifanio, I.;Intervention in prediction measure: A new approach to assessing variable importance for random forests;Background: Random forests are a popular method in many fields since they can be successfully applied to complex data, with a small sample size, complex interactions and correlations, mixed type predictors, etc. Furthermore, they provide variable importance measures that aid qualitative interpretation and also the selection of relevant predictors. However, most of these measures rely on the choice of a performance measure. But measures of prediction performance are not unique or there is not even a clear definition, as in the case of multivariate response random forests. Methods: A new alternative importance measure, called Intervention in Prediction Measure, is investigated. It depends on the structure of the trees, without depending on performance measures. It is compared with other well-known variable importance measures in different contexts, such as a classification problem with variables of different types, another classification problem with correlated predictor variables, and problems with multivariate responses and predictors of different types. Results: Several simulation studies are carried out, showing the new measure to be very competitive. In addition, it is applied in two well-known bioinformatics applications previously used in other papers. Improvements in performance are also provided for these applications by the use of this new measure. Conclusions: This new measure is expressed as a percentage, which makes it attractive in terms of interpretability. It can be used with new observations. It can be defined globally, for each class (in a classification problem) and case-wise. It can easily be computed for any kind of response, including multivariate responses. Furthermore, it can be used with any algorithm employed to grow each individual tree. It can be used in place of (or in addition to) other variable importance measures. © 2017 The Author(s).;BMC Bioinformatics;10.1186/s12859-017-1650-8;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018787767&doi=10.1186%2fs12859-017-1650-8&partnerID=40&md5=a0fa49f9d4ae420b0a085a8aef5fb499;2017;17/02/2022 13:40;17/02/2022 13:40;;;Q8NNLGGA;journalArticle;;1;18;;Scopus;Publisher: BioMed Central Ltd.;"<p>Cited By :23</p>; <p>Export Date: 17 February 2022</p>";"algorithm; Algorithms; bioinformatics; Bioinformatics applications; biology; classification; Computational Biology; Conditional inference; decision tree; Decision trees; Decision Trees; Feature extraction; Forecasting; Forestry; Models, Statistical; multivariate analysis; Multivariate Analysis; Multivariate response; prediction; Prediction measures; Prediction performance; predictor variable; Predictor variables; procedures; random forest; Random forests; simulation; statistical model; Variable importances"
Preprint;ArXiv;1;search;1;2018;SHAP;Hybrid;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;All;Hybrid;;Mixte;Mixte;1;Python;Python;822;"Lundberg, Scott M.; Erion, Gabriel G.; Lee, Su-In";Consistent Individualized Feature Attribution for Tree Ensembles;"Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique ""supervised"" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.";ArXiv;;http://arxiv.org/abs/1802.03888;2018-02;18/02/2022 10:44;18/02/2022 10:44;;;NC89MJ3J;journalArticle;;;;;;;;
Preprint;ArXiv;1;search;1;2018;MAPLE ;Hybrid;;model-agnostic ;post hoc;Post-hoc;Features oriented;F.O.;Global overview + Local explanation;G.O. + L.E.;Regression;Reg.;;Continuous;Mixte;1;Python;Python;113;"Plumb, Gregory; Molitor, Denali; Talwalkar, Ameet";Model agnostic supervised local explanations;Model interpretability is an increasingly important component of practical machine learning. Some of the most common forms of interpretability systems are example-based, local, and global explanations. One of the main challenges in interpretability is designing explanation systems that can capture aspects of each of these explanation types, in order to develop a more thorough understanding of the model. We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. Specifically, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests and that it produces more faithful local explanations than LIME, a popular interpretability system. Second, MAPLE provides both example-based and local explanations and can detect global patterns, which allows it to diagnose limitations in its local explanations.;Advances in Neural Information Processing Systems;;;2018;18/02/2022 10:44;18/02/2022 10:44;2515-2524;;QLPLEPLD;journalArticle;;;2018-December;;;Publisher: Neural information processing systems foundation;;
Preprint;ArXiv;1;search;1;2019;ADD-Lib;Hybrid;;model-specific;post hoc;Post-hoc;Template/Web interface;T./W.I.;Global overview + Pattern discovery;G.O. + P.D.;Multiclass;M.C.;;Mixte;Visual graphs;1;Cinco/Pyro;Others;6;"Gossen, Frederik; Murtovi, Alnis; Zweihoff, Philip; Steffen, Bernhard";ADD-Lib: Decision Diagrams in Practice;In the paper, we present the ADD-Lib, our efficient and easy to use framework for Algebraic Decision Diagrams (ADDs). The focus of the ADD-Lib is not so much on its efficient implementation of individual operations, which are taken by other established ADD frameworks, but its ease and flexibility, which arise at two levels: the level of individual ADD-tools, which come with a dedicated user-friendly web-based graphical user interface, and at the meta level, where such tools are specified. Both levels are described in the paper: the meta level by explaining how we can construct an ADD-tool tailored for Random Forest refinement and evaluation, and the accordingly generated Web-based domain-specific tool, which we also provide as an artifact for cooperative experimentation. In particular, the artifact allows readers to combine a given Random Forest with their own ADDs regarded as expert knowledge and to experience the corresponding effect.;ArXiv;;http://arxiv.org/abs/1912.11308;2019-12;18/02/2022 10:43;18/02/2022 10:43;;;7KIJUBCP;journalArticle;;;;;;;;
Preprint;ArXiv;1;search;1;2019;TreeExplainer;Hybrid;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;All;Hybrid;;Mixte;Visual graphs;1;Python;Python;186;"Lundberg, Scott M.; Erion, Gabriel; Chen, Hugh; DeGrave, Alex; Prutkin, Jordan M.; Nair, Bala; Katz, Ronit; Himmelfarb, Jonathan; Bansal, Nisha; Lee, Su-In";Explainable AI for Trees: From Local Explanations to Global Understanding;Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.;ArXiv;;http://arxiv.org/abs/1905.04610;2019-05;18/02/2022 10:44;18/02/2022 10:44;;;9G3BQMBL;journalArticle;;;;;;;;
Journal paper;IEEEXplore;1;search;1;2019;iForest;Hybrid;;model-specific;post hoc;Post-hoc;Features oriented + Sample similarity-based;F.O. + S.S.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Binaryclass;B.C;;Mixte;Mixte;1;Python/D3;Python;98;"X. Zhao; Y. Wu; D. L. Lee; W. Cui";iForest: Interpreting Random Forests via Visual Analytics;As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.;IEEE Transactions on Visualization and Computer Graphics;10.1109/TVCG.2018.2864475;;2019-01;17/02/2022 14:14;17/02/2022 14:14;407-416;1941-0506;6RMX7SDN;journalArticle;;1;25;IEEE Transactions on Visualization and Computer Graphics;;;;
Journal paper;SCOPUS;1;search;1;2019;inTrees;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;cont+discretization;Mixte;Mixte;1;R;R;195;Deng, H.;Interpreting tree ensembles with inTrees;Tree ensembles such as random forests and boosted trees are accurate but difficult to understand. In this work, we provide the interpretable trees (inTrees) framework that extracts, measures, prunes, selects, and summarizes rules from a tree ensemble, and calculates frequent variable interactions. The inTrees framework can be applied to multiple types of tree ensembles, e.g., random forests, regularized random forests, and boosted trees. We implemented the inTrees algorithms in the “inTrees” R package. © 2018, Springer Nature Switzerland AG.;International Journal of Data Science and Analytics;10.1007/s41060-018-0144-8;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088166403&doi=10.1007%2fs41060-018-0144-8&partnerID=40&md5=841342a3e3414997c4f6850d61929a2e;2019;17/02/2022 13:39;17/02/2022 13:39;277-287;;XNEDBYWP;journalArticle;;4;7;;Scopus;Publisher: Springer Science and Business Media Deutschland GmbH;"<p>Cited By :57</p>; <p>Export Date: 17 February 2022</p>";"Decision trees; Forestry; Intrees; Random forests; Regularized random forests; Tree ensembles; Variable interaction"
Preprint;ArXiv;1;search;1;2019;SSFI ;Local;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Local explanation;L.E.;All;Hybrid;;Mixte;Mixte;1;Not mentioned;Not mentioned;2;"Gatto, Joseph; Lanka, Ravi; Iwashita, Yumi; Stoica, Adrian";Single Sample Feature Importance: An Interpretable Algorithm for Low-Level Feature Analysis;Have you ever wondered how your feature space is impacting the prediction of a specific sample in your dataset? In this paper, we introduce Single Sample Feature Importance (SSFI), which is an interpretable feature importance algorithm that allows for the identification of the most important features that contribute to the prediction of a single sample. When a dataset can be learned by a Random Forest classifier or regressor, SSFI shows how the Random Forest's prediction path can be utilized for low-level feature importance calculation. SSFI results in a relative ranking of features, highlighting those with the greatest impact on a data point's prediction. We demonstrate these results both numerically and visually on four different datasets.;ArXiv;;http://arxiv.org/abs/1911.11901;2019-11;18/02/2022 10:44;18/02/2022 10:44;;;CL2QUFS6;journalArticle;;;;;;;;
Journal paper;SCOPUS;1;search;1;2019;SMD;Global;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview;G.O.;All;Hybrid;;Mixte;Mixte;1;R;R;20;"Seifert, S.; Gundlach, S.; Szymczak, S.";Surrogate minimal depth as an importance measure for variables in random forests;Motivation: It has been shown that the machine learning approach random forest can be successfully applied to omics data, such as gene expression data, for classification or regression and to select variables that are important for prediction. However, the complex relationships between predictor variables, in particular between causal predictor variables, make the interpretation of currently applied variable selection techniques difficult. Results: Here we propose a new variable selection approach called surrogate minimal depth (SMD) that incorporates surrogate variables into the concept of minimal depth (MD) variable importance. Applying SMD, we show that simulated correlation patterns can be reconstructed and that the increased consideration of variable relationships improves variable selection. When compared with existing state-of-the-art methods and MD, SMD has higher empirical power to identify causal variables while the resulting variable lists are equally stable. In conclusion, SMD is a promising approach to get more insight into the complex interplay of predictor variables and outcome in a high-dimensional data setting. Availability and implementation: https://github.com/StephanSeifert/SurrogateMinimalDepth. Supplementary information: Supplementary data are available at Bioinformatics online. © 2019 The Author(s). Published by Oxford University Press.;Bioinformatics;10.1093/bioinformatics/btz149;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070378016&doi=10.1093%2fbioinformatics%2fbtz149&partnerID=40&md5=c319f260cf4ff947b4ad3301b4dad96d;2019;17/02/2022 13:41;17/02/2022 13:41;3663-3671;;BHKESD9K;journalArticle;;19;35;;Scopus;Publisher: Oxford University Press;"<p>Cited By :9</p>; <p>Export Date: 17 February 2022</p>";"article; bioinformatics; machine learning; Machine Learning; predictor variable; random forest; simulation"
Journal paper;SCOPUS;1;search;1;2020;IRFRE;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Binaryclass;B.C;;Mixte;Text/statistics;1;Not mentioned;Not mentioned;53;"Wang, S.; Wang, Y.; Wang, D.; Yin, Y.; Wang, Y.; Jin, Y.";An improved random forest-based rule extraction method for breast cancer diagnosis;Breast cancer has been becoming the main cause of death in women all around the world. An accurate and interpretable method is necessary for diagnosing patients with breast cancer for well-performed treatment. Nowadays, a great many of ensemble methods have been widely applied to breast cancer diagnosis, capable of achieving high accuracy, such as Random Forest. However, they are black-box methods which are unable to explain the reasons behind the diagnosis. To surmount this limitation, a rule extraction method named improved Random Forest (RF)-based rule extraction (IRFRE) method is developed to derive accurate and interpretable classification rules from a decision tree ensemble for breast cancer diagnosis. Firstly, numbers of decision tree models are constructed using Random Forest to generate abundant decision rules available. And then a rule extraction approach is devised to detach decision rules from the trained trees. Finally, an improved multi-objective evolutionary algorithm (MOEA) is employed to seek for an optimal rule predictor where the constituent rule set is the best trade-off between accuracy and interpretability. The developed method is evaluated on three breast cancer data sets, i.e., the Wisconsin Diagnostic Breast Cancer (WDBC) dataset, Wisconsin Original Breast Cancer (WOBC) dataset, and Surveillance, Epidemiology and End Results (SEER) breast cancer dataset. The experimental results demonstrate that the developed method can primely explain the black-box methods and outperform several popular single algorithms, ensemble learning methods, and rule extraction methods from the view of accuracy and interpretability. What is more, the proposed method can be popularized to other cancer diagnoses in practice, which provides an option to a more interpretable, more accurate cancer diagnosis process. © 2019 Elsevier B.V.;Applied Soft Computing Journal;10.1016/j.asoc.2019.105941;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076576728&doi=10.1016%2fj.asoc.2019.105941&partnerID=40&md5=a4a462f40ec291c7d6ad164cb6a8b623;2020;17/02/2022 13:39;17/02/2022 13:39;;;XXPYL3SG;journalArticle;;;86;;Scopus;Publisher: Elsevier Ltd;"<p>Cited By :24</p>; <p>Export Date: 17 February 2022</p>";"Breast cancer diagnosis; Computer aided diagnosis; Decision trees; Diseases; Economic and social effects; Evolutionary algorithms; Extraction; Interpretability; Learning systems; MOEAs; Patient treatment; Random forests; Rule extraction"
Journal paper;SCOPUS;1;search;1;2020;Explainer;Local;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Local explanation;L.E.;Multiclass;M.C.;;Mixte;Text/statistics;1;Matlab;Others;15;"Kopp, M.; Pevný, T.; Hole?a, M.";Anomaly explanation with random forests;Anomaly detection has become an important topic in many domains with many different solutions proposed until now. Despite that, there are only a few anomaly detection methods trying to explain how the sample differs from the rest. This work contributes to filling this gap because knowing why a sample is considered anomalous is critical in many application domains. The proposed solution uses a specific type of random forests to extract rules explaining the difference, which are then filtered and presented to the user as a set of classification rules sharing the same consequent, or as the equivalent rule with an antecedent in a disjunctive normal form. The quality of that solution is documented by comparison with the state of the art algorithms on 34 real-world datasets. © 2020;Expert Systems with Applications;10.1016/j.eswa.2020.113187;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078848410&doi=10.1016%2fj.eswa.2020.113187&partnerID=40&md5=95fba46806b469f0534902433f5848b9;2020;17/02/2022 13:40;17/02/2022 13:40;;;WNGRIBK5;journalArticle;;;149;;Scopus;Publisher: Elsevier Ltd;"<p>Cited By :9</p>; <p>Export Date: 17 February 2022</p>";"Anomaly detection; Anomaly detection methods; Anomaly explanation; Classification rules; Decision trees; Disjunctive normal form; Feature extraction; Random forests; Real-world datasets; State-of-the-art algorithms"
Journal paper;SCOPUS;1;search;1;2020;CHIRPS;Local;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Local explanation;L.E.;Multiclass;M.C.;cont+discretization;Mixte;Text/statistics;1;Python;Python;19;"Hatwell, J.; Gaber, M.M.; Azad, R.M.A.";CHIRPS: Explaining random forest classification;"Modern machine learning methods typically produce “black box” models that are opaque to interpretation. Yet, their demand has been increasing in the Human-in-the-Loop processes, that is, those processes that require a human agent to verify, approve or reason about the automated decisions before they can be applied. To facilitate this interpretation, we propose Collection of High Importance Random Path Snippets (CHIRPS); a novel algorithm for explaining random forest classification per data instance. CHIRPS extracts a decision path from each tree in the forest that contributes to the majority classification, and then uses frequent pattern mining to identify the most commonly occurring split conditions. Then a simple, conjunctive form rule is constructed where the antecedent terms are derived from the attributes that had the most influence on the classification. This rule is returned alongside estimates of the rule’s precision and coverage on the training data along with counter-factual details. An experimental study involving nine data sets shows that classification rules returned by CHIRPS have a precision at least as high as the state of the art when evaluated on unseen data (0.91–0.99) and offer a much greater coverage (0.04–0.54). Furthermore, CHIRPS uniquely controls against under- and over-fitting solutions by maximising novel objective functions that are better suited to the local (per instance) explanation setting. © 2020, The Author(s).";Artificial Intelligence Review;10.1007/s10462-020-09833-6;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086030855&doi=10.1007%2fs10462-020-09833-6&partnerID=40&md5=978eea90411b2975e4af57b03fd47db5;2020;17/02/2022 13:41;17/02/2022 13:41;5747-5788;;7VZ7ATNG;journalArticle;;8;53;;Scopus;Publisher: Springer Science+Business Media B.V.;"<p>Cited By :7</p>; <p>Export Date: 17 February 2022</p>";"Chirp modulation; Classification (of information); Classification rules; Decision trees; Frequent pattern mining; Human-in-the-loop; Learning systems; Modern machines; Novel algorithm; Objective functions; Random forest classification; Random forests; State of the art"
Journal paper;SCOPUS;1;search;1;2020;permimp;Global;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview;G.O.;All;Hybrid;cont+discretization;Mixte;Mixte;1;R;R;30;"Debeer, D.; Strobl, C.";Conditional permutation importance revisited;Background: Random forest based variable importance measures have become popular tools for assessing the contributions of the predictor variables in a fitted random forest. In this article we reconsider a frequently used variable importance measure, the Conditional Permutation Importance (CPI). We argue and illustrate that the CPI corresponds to a more partial quantification of variable importance and suggest several improvements in its methodology and implementation that enhance its practical value. In addition, we introduce the threshold value in the CPI algorithm as a parameter that can make the CPI more partial or more marginal. Results: By means of extensive simulations, where the original version of the CPI is used as the reference, we examine the impact of the proposed methodological improvements. The simulation results show how the improved CPI methodology increases the interpretability and stability of the computations. In addition, the newly proposed implementation decreases the computation times drastically and is more widely applicable. The improved CPI algorithm is made freely available as an add-on package to the open-source software R. Conclusion: The proposed methodology and implementation of the CPI is computationally faster and leads to more stable results. It has a beneficial impact on practical research by making random forest analyses more interpretable. © 2020 The Author(s).;BMC Bioinformatics;10.1186/s12859-020-03622-2;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088048949&doi=10.1186%2fs12859-020-03622-2&partnerID=40&md5=73ca38efc8ae18551c101301f487c9f7;2020;17/02/2022 13:39;17/02/2022 13:39;;;4YV4T377;journalArticle;;1;21;;Scopus;Publisher: BioMed Central;"<p>Cited By :10</p>; <p>Export Date: 17 February 2022</p>";"algorithm; Algorithms; article; Computation time; computer simulation; Computer Simulation; Decision trees; Extensive simulations; Interpretability; Open source software; Open systems; Predictor variables; random forest; Random forests; simulation; software; Software; Threshold-value; Variable importances"
Journal paper;SCOPUS;1;search;1;2020;forest_based_tree;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Mixte;Mixte;1;Python;Python;51;"Sagi, O.; Rokach, L.";Explainable decision forest: Transforming a decision forest into an interpretable tree;"Decision forests are considered the best practice in many machine learning challenges, mainly due to their superior predictive performance. However, simple models like decision trees may be preferred over decision forests in cases in which the generated predictions must be efficient or interpretable (e.g. in insurance or health-related use cases). This paper presents a novel method for transforming a decision forest into an interpretable decision tree, which aims at preserving the predictive performance of decision forests while enabling efficient classifications that can be understood by humans. This is done by creating a set of rule conjunctions that represent the original decision forest; the conjunctions are then hierarchically organized to form a new decision tree. We evaluate the proposed method on 33 UCI datasets and show that the resulting model usually approximates the ROC AUC gained by random forest while providing an interpretable decision path for each classification. © 2020 Elsevier B.V.";Information Fusion;10.1016/j.inffus.2020.03.013;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083315261&doi=10.1016%2fj.inffus.2020.03.013&partnerID=40&md5=8fdcb4bbeaffca7565028650c74d85b9;2020;17/02/2022 13:41;17/02/2022 13:41;124-138;;ITG9M6HM;journalArticle;;;61;;Scopus;Publisher: Elsevier B.V.;"<p>Cited By :25</p>; <p>Export Date: 17 February 2022</p>";"Best practices; Classification (of information); Decision forest; Decision paths; Decision trees; Learning algorithms; Predictive performance; Set of rules; Uci datasets"
Preprint;ArXiv;1;search;1;2020;rfVarImpOOB;Global;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview;G.O.;All;Hybrid;;Mixte;Mixte;1;R;R;2;Loecher, Markus;Unbiased variable importance for random forests;We attempt to give a unifying view of the various recent attempts to (i) improve the interpretability of tree-based models and (ii) debias the the default variable-importance measure in random Forests, Gini importance. In particular, we demonstrate a common thread among the out-of-bag based bias correction methods and their connection to local explanation for trees. In addition, we point out a bias caused by the inclusion of inbag data in the newly developed explainable AI for trees algorithms.;ArXiv;;http://arxiv.org/abs/2003.12043;2020-03;18/02/2022 10:43;18/02/2022 10:43;;;C6CVURR6;journalArticle;;;;;;;;
Journal paper;SCOPUS;1;search;1;2020;RF-OCSE;Local;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Local explanation;L.E.;Binaryclass;B.C;;Mixte;Mixte;1;Python;Python;28;"Fernández, R.R.; Martín de Diego, I.; Aceña, V.; Fernández-Isabel, A.; Moguerza, J.M.";Random forest explainability using counterfactual sets;Nowadays, Machine Learning (ML) models are becoming ubiquitous in today's society, supporting people with their day-to-day decisions. In this context, Explainable ML is a field of Artificial Intelligence (AI) that focuses on making predictive models and their decisions interpretable by humans, enabling people to trust predictive models and to understand the underlying processes. A counterfactual is an effective type of Explainable ML technique that explains predictions by describing the changes needed in a sample to flip the outcome of the prediction. In this paper, we introduce counterfactual sets, an explanation approach that uses a set of counterfactuals to explain a prediction rather than a single counterfactual, by defining a sub-region of the feature space where the counterfactual holds. A method to extract counterfactual sets from a Random Forest (RF), the RandomForestOptimalCounterfactualSetExtractor(RF?OCSE), is presented. The method is based on a partial fusion of tree predictors from a RF into a single Decision Tree (DT) using a modification of the CART algorithm, and it obtains a counterfactual set that contains the optimal counterfactual. The proposal is validated through several experiments against existing alternatives on ten well-known datasets by comparing the percentage of valid counterfactuals, distance to the factual sample, and counterfactual sets quality. © 2020;Information Fusion;10.1016/j.inffus.2020.07.001;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087720092&doi=10.1016%2fj.inffus.2020.07.001&partnerID=40&md5=9cd813087edb044872044bc4d6de6af3;2020;17/02/2022 13:41;17/02/2022 13:41;196-207;;WL59YBPA;journalArticle;;;63;;Scopus;Publisher: Elsevier B.V.;"<p>Cited By :13</p>; <p>Export Date: 17 February 2022</p>";"Artificial intelligence; CART algorithms; Counterfactuals; Decision trees; Feature space; Forecasting; Predictive models; Random forests; Single decision; Sub-regions"
Preprint;ArXiv;1;search;1;2020;MIRCO;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Mixte;Text/statistics;1;Python;Python;5;"Birbil, S. Ilker; Edali, Mert; Yuceoglu, Birol";Rule Covering for Interpretation and Boosting;We propose two algorithms for interpretation and boosting of tree-based ensemble methods. Both algorithms make use of mathematical programming models that are constructed with a set of rules extracted from an ensemble of decision trees. The objective is to obtain the minimum total impurity with the least number of rules that cover all the samples. The first algorithm uses the collection of decision trees obtained from a trained random forest model. Our numerical results show that the proposed rule covering approach selects only a few rules that could be used for interpreting the random forest model. Moreover, the resulting set of rules closely matches the accuracy level of the random forest model. Inspired by the column generation algorithm in linear programming, our second algorithm uses a rule generation scheme for boosting decision trees. We use the dual optimal solutions of the linear programming models as sample weights to obtain only those rules that would improve the accuracy. With a computational study, we observe that our second algorithm performs competitively with the other well-known boosting methods. Our implementations also demonstrate that both algorithms can be trivially coupled with the existing random forest and decision tree packages.;ArXiv;;http://arxiv.org/abs/2007.06379;2020-07;18/02/2022 10:43;18/02/2022 10:43;;;V5A2483R;journalArticle;;;;;;;;
Preprint;ArXiv;1;search;1;2020;TREX;Hybrid;;model-specific;post hoc;Post-hoc;Sample similarity-based;S.S;Pattern discovery;P.D.;Binaryclass;B.C;;Mixte;Visual graphs;1;Python;Python;2;"Brophy, Jonathan; Lowd, Daniel";TREX: Tree-Ensemble Representer-Point Explanations;"How can we identify the training examples that contribute most to the prediction of a tree ensemble? In this paper, we introduce TREX, an explanation system that provides instance-attribution explanations for tree ensembles, such as random forests and gradient boosted trees. TREX builds on the representer point framework previously developed for explaining deep neural networks. Since tree ensembles are non-differentiable, we define a kernel that captures the structure of the specific tree ensemble. By using this kernel in kernel logistic regression or a support vector machine, TREX builds a surrogate model that approximates the original tree ensemble. The weights in the kernel expansion of the surrogate model are used to define the global or local importance of each training example. Our experiments show that TREX's surrogate model accurately approximates the tree ensemble; its global importance weights are more effective in dataset debugging than the previous state-of-the-art; its explanations identify the most influential samples better than alternative methods under the remove and retrain evaluation framework; it runs orders of magnitude faster than alternative methods; and its local explanations can identify and explain errors due to domain mismatch.";ArXiv;;http://arxiv.org/abs/2009.05530;2020-09;18/02/2022 10:43;18/02/2022 10:43;;;KQ7KYS95;journalArticle;;;;;;;;
Journal paper;SCOPUS;1;search;1;2021;smit21;Hybrid;;model-specific;post hoc;Post-hoc;Template/Web interface;T./W.I.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Binaryclass;B.C;;Mixte;Mixte;2->1;R;R;1;"Smith, M.; Alvarez, F.";A machine learning research template for binary classification problems and shapley values integration[Formula presented];This paper documents published code which can help facilitate researchers with binary classification problems and interpret the results from a number of Machine Learning models. The original paper was published in Expert Systems with Applications and this paper documents the code and work-flow with a special interest being paid to Shapley values as a means to interpret Machine Learning predictions. The Machine Learning models used are, Naive Bayes, Logistic Regression, Random Forest, adaBoost, Classification Tree, Light GBM and XGBoost. © 2021 The Author(s);Software Impacts;10.1016/j.simpa.2021.100074;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115879190&doi=10.1016%2fj.simpa.2021.100074&partnerID=40&md5=af0206aed83fe1a95a47bd0820aefc63;2021;17/02/2022 13:40;17/02/2022 13:40;;;KKE7KVB5;journalArticle;;;8;;Scopus;Publisher: Elsevier B.V.;"<p>Cited By :1</p>; <p>Export Date: 17 February 2022</p>";
Journal paper;SCOPUS;1;search;1;2021;RF-DIMLP;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Binaryclass;B.C;;Mixte;Text/statistics;1;Python;Python;0;Bologna, G.;A rule extraction technique applied to ensembles of neural networks, random forests, and gradient-boosted trees;In machine learning, ensembles of models based on Multi-Layer Perceptrons (MLPs) or decision trees are considered successful models. However, explaining their responses is a complex problem that requires the creation of new methods of interpretation. A natural way to explain the classifications of the models is to transform them into propositional rules. In this work, we focus on random forests and gradient-boosted trees. Specifically, these models are converted into an ensemble of interpretable MLPs from which propositional rules are produced. The rule extraction method presented here allows one to precisely locate the discriminating hyperplanes that constitute the antecedents of the rules. In experiments based on eight classification problems, we compared our rule extraction technique to “Skope-Rules” and other state-of-the-art techniques. Experiments were performed with ten-fold cross-validation trials, with propositional rules that were also generated from ensembles of interpretable MLPs. By evaluating the characteristics of the extracted rules in terms of complexity, fidelity, and accuracy, the results obtained showed that our rule extraction technique is competitive. To the best of our knowledge, this is one of the few works showing a rule extraction technique that has been applied to both ensembles of decision trees and neural networks. © 2021 by the author. Licensee MDPI, Basel, Switzerland.;Algorithms;10.3390/a14120339;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120002972&doi=10.3390%2fa14120339&partnerID=40&md5=02f70cb1909fca9a7671a17a55da1cfc;2021;17/02/2022 13:41;17/02/2022 13:41;;;BEAXAA85;journalArticle;;12;14;;Scopus;Publisher: MDPI;<p>Export Date: 17 February 2022</p>;"Bagging; Boosting; Complex networks; Decision trees; Ensemble; Ensemble of models; Extraction; Extraction techniques; Forestry; Model explanation; Model-based OPC; Multilayers perceptrons; Neural-networks; Rules extraction"
Preprint;ArXiv;1;search;1;2021;blan21;Local;;model-specific;post hoc;Post-hoc;Sample similarity-based;S.S;Local explanation;L.E.;All;Hybrid;;Mixte;Mixte;1;R;R;0;Blanchart, Pierre;An exact counterfactual-example-based approach to tree-ensemble models interpretability;Explaining the decisions of machine learning models is becoming a necessity in many areas where trust in ML models decision is key to their accreditation/adoption. The ability to explain models decisions also allows to provide diagnosis in addition to the model decision, which is highly valuable in scenarios such as fault detection. Unfortunately, high-performance models do not exhibit the necessary transparency to make their decisions fully understandable. And the black-boxes approaches, which are used to explain such model decisions, suffer from a lack of accuracy in tracing back the exact cause of a model decision regarding a given input. Indeed, they do not have the ability to explicitly describe the decision regions of the model around that input, which is necessary to determine what influences the model towards one decision or the other. We thus asked ourselves the question: is there a category of high-performance models among the ones currently used for which we could explicitly and exactly characterise the decision regions in the input feature space using a geometrical characterisation? Surprisingly we came out with a positive answer for any model that enters the category of tree ensemble models, which encompasses a wide range of high-performance models such as XGBoost, LightGBM, random forests ... We could derive an exact geometrical characterisation of their decision regions under the form of a collection of multidimensional intervals. This characterisation makes it straightforward to compute the optimal counterfactual (CF) example associated with a query point. We demonstrate several possibilities of the approach, such as computing the CF example based only on a subset of features. This allows to obtain more plausible explanations by adding prior knowledge about which variables the user can control. An adaptation to CF reasoning on regression problems is also envisaged.;ArXiv;;http://arxiv.org/abs/2105.14820;2021-05;18/02/2022 10:43;18/02/2022 10:43;;;3YL9JTSX;journalArticle;;;;;;;;
Preprint;ArXiv;1;search;1;2021;LionForests;Local;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Local explanation;L.E.;All;Hybrid;;Mixte;Mixte;1;Python;Python;0;"Mollas, Ioannis; Bassiliades, Nick; Tsoumakas, Grigorios";Conclusive Local Interpretation Rules for Random Forests;In critical situations involving discrimination, gender inequality, economic damage, and even the possibility of casualties, machine learning models must be able to provide clear interpretations for their decisions. Otherwise, their obscure decision-making processes can lead to socioethical issues as they interfere with people's lives. In the aforementioned sectors, random forest algorithms strive, thus their ability to explain themselves is an obvious requirement. In this paper, we present LionForests, which relies on a preliminary work of ours. LionForests is a random forest-specific interpretation technique, which provides rules as explanations. It is applicable from binary classification tasks to multi-class classification and regression tasks, and it is supported by a stable theoretical background. Experimentation, including sensitivity analysis and comparison with state-of-the-art techniques, is also performed to demonstrate the efficacy of our contribution. Finally, we highlight a unique property of LionForests, called conclusiveness, that provides interpretation validity and distinguishes it from previous techniques.;ArXiv;;http://arxiv.org/abs/2104.06040;2021-04;18/02/2022 10:43;18/02/2022 10:43;;;7FG5NUN7;journalArticle;;;;;;;;
Preprint;ArXiv;1;search;1;2021;ACV;Local;;model-agnostic ;post hoc;Post-hoc;Features oriented;F.O.;Local explanation;L.E.;All;Hybrid;;Mixte;Mixte;1;Python;Python;1;"Amoukou, Salim I.; Brunel, Nicolas J. B";Consistent Sufficient Explanations and Minimal Local Rules for explaining regression and classification models;To explain the decision of any model, we extend the notion of probabilistic Sufficient Explanations (P-SE). For each instance, this approach selects the minimal subset of features that is sufficient to yield the same prediction with high probability, while removing other features. The crux of P-SE is to compute the conditional probability of maintaining the same prediction. Therefore, we introduce an accurate and fast estimator of this probability via random Forests for any data $(\boldsymbol{X}, Y)$ and show its efficiency through a theoretical analysis of its consistency. As a consequence, we extend the P-SE to regression problems. In addition, we deal with non-binary features, without learning the distribution of $X$ nor having the model for making predictions. Finally, we introduce local rule-based explanations for regression/classification based on the P-SE and compare our approaches w.r.t other explainable AI methods. These methods are publicly available as a Python package at \url{www.github.com/salimamoukou/acv00}.;ArXiv;;http://arxiv.org/abs/2111.04658;2021-11;18/02/2022 10:43;18/02/2022 10:43;;;AUEMTUXJ;journalArticle;;;;;;;;
Preprint;ArXiv;1;search;1;2021;ER-SHAP;Hybrid;;model-agnostic ;post hoc;Post-hoc;Features oriented;F.O.;Global overview + Local explanation;G.O. + L.E.;All;Hybrid;;Mixte;Visual graphs;2->1;Not mentioned;Not mentioned;3;"Utkin, Lev V.; Konstantinov, Andrei V.";Ensembles of Random SHAPs;"Ensemble-based modifications of the well-known SHapley Additive exPlanations (SHAP) method for the local explanation of a black-box model are proposed. The modifications aim to simplify SHAP which is computationally expensive when there is a large number of features. The main idea behind the proposed modifications is to approximate SHAP by an ensemble of SHAPs with a smaller number of features. According to the first modification, called ER-SHAP, several features are randomly selected many times from the feature set, and Shapley values for the features are computed by means of ""small"" SHAPs. The explanation results are averaged to get the final Shapley values. According to the second modification, called ERW-SHAP, several points are generated around the explained instance for diversity purposes, and results of their explanation are combined with weights depending on distances between points and the explained instance. The third modification, called ER-SHAP-RF, uses the random forest for preliminary explanation of instances and determining a feature probability distribution which is applied to selection of features in the ensemble-based procedure of ER-SHAP. Many numerical experiments illustrating the proposed modifications demonstrate their efficiency and properties for local explanation.";ArXiv;;http://arxiv.org/abs/2103.03302;2021-03;18/02/2022 10:43;18/02/2022 10:43;;;2PIN4I53;journalArticle;;;;;;;;
Journal paper;IEEEXplore;1;search;1;2021;ExMatrix;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Local explanation;G.O. + L.E.;Multiclass;M.C.;;Mixte;Visual graphs;1;Python;Python;35;"M. P. Neto; F. V. Paulovich";Explainable Matrix - Visualization for Global and Local Interpretability of Random Forest Classification Ensembles;Over the past decades, classification models have proven to be essential machine learning tools given their potential and applicability in various domains. In these years, the north of the majority of the researchers had been to improve quantitative metrics, notwithstanding the lack of information about models' decisions such metrics convey. This paradigm has recently shifted, and strategies beyond tables and numbers to assist in interpreting models' decisions are increasing in importance. Part of this trend, visualization techniques have been extensively used to support classification models' interpretability, with a significant focus on rule-based models. Despite the advances, the existing approaches present limitations in terms of visual scalability, and the visualization of large and complex models, such as the ones produced by the Random Forest (RF) technique, remains a challenge. In this paper, we propose Explainable Matrix (ExMatrix), a novel visualization method for RF interpretability that can handle models with massive quantities of rules. It employs a simple yet powerful matrix-like visual metaphor, where rows are rules, columns are features, and cells are rules predicates, enabling the analysis of entire models and auditing classification results. ExMatrix applicability is confirmed via different examples, showing how it can be used in practice to promote RF models interpretability.;IEEE Transactions on Visualization and Computer Graphics;10.1109/TVCG.2020.3030354;;2021-02;17/02/2022 14:14;17/02/2022 14:14;1427-1437;1941-0506;ZBWUWBZ9;journalArticle;;2;27;IEEE Transactions on Visualization and Computer Graphics;;;;
Preprint;ArXiv;1;search;1;2021;verd21;Hybrid;;model-agnostic ;post hoc;Post-hoc;Features oriented;F.O.;Global overview + Local explanation;G.O. + L.E.;Regression;Reg.;;Continuous;Mixte;1;Not mentioned;Not mentioned;0;"Verdinelli, Isabella; Wasserman, Larry";Forest Guided Smoothing;We use the output of a random forest to define a family of local smoothers with spatially adaptive bandwidth matrices. The smoother inherits the flexibility of the original forest but, since it is a simple, linear smoother, it is very interpretable and it can be used for tasks that would be intractable for the original forest. This includes bias correction, confidence intervals, assessing variable importance and methods for exploring the structure of the forest. We illustrate the method on some synthetic examples and on data related to Covid-19.;ArXiv;;http://arxiv.org/abs/2103.05092;2021-03;18/02/2022 10:43;18/02/2022 10:43;;;G68F8KK7;journalArticle;;;;;;;;
Preprint;ArXiv;1;search;1;2021;RFxpl;Local;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Local explanation;L.E.;Multiclass;M.C.;;Mixte;Text/statistics;2->1;Python;Python;14;"Izza, Yacine; Marques-Silva, Joao";On Explaining Random Forests with SAT;Random Forest (RFs) are among the most widely used Machine Learning (ML) classifiers. Even though RFs are not interpretable, there are no dedicated non-heuristic approaches for computing explanations of RFs. Moreover, there is recent work on polynomial algorithms for explaining ML models, including naive Bayes classifiers. Hence, one question is whether finding explanations of RFs can be solved in polynomial time. This paper answers this question negatively, by proving that computing one PI-explanation of an RF is D^P-hard. Furthermore, the paper proposes a propositional encoding for computing explanations of RFs, thus enabling finding PI-explanations with a SAT solver. This contrasts with earlier work on explaining boosted trees (BTs) and neural networks (NNs), which requires encodings based on SMT/MILP. Experimental results, obtained on a wide range of publicly available datasets, demonstrate that the proposed SAT-based approach scales to RFs of sizes common in practical applications. Perhaps more importantly, the experimental results demonstrate that, for the vast majority of examples considered, the SAT-based approach proposed in this paper significantly outperforms existing heuristic approaches.;ArXiv;10.24963/ijcai.2021/356;;2021-08;18/02/2022 10:43;18/02/2022 10:43;2584-2591;;3AC9DVCH;journalArticle;;;;;;Publisher: International Joint Conferences on Artificial Intelligence;;
Journal paper;ScienceDirect;1;search;1;2021;StratPD;Global;;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview;G.O.;Regression;Reg.;;Mixte;Visual graphs;2->1;Python;Python;1;"Parr, Terence; Wilson, James D.";Partial dependence through stratification;Partial dependence curves (FPD) are commonly used to explain feature importance once a supervised learning model has been fitted to data. However, it is common for the same partial dependence algorithm to give meaningfully different curves for different supervised models, even when the algorithm is applied to the same data. As a result, it is difficult to distinguish between model artifacts and true relationships in the data. In this paper, we contribute metods for computing partial dependence curves, for both numerical (StratPD) and categorical explanatory variables (CatStratPD), that work directly from training data rather than the predictions of a fitted model. Our methods provide a direct estimate of partial dependence, and rely on approximating the partial derivative of an unknown regression function. We investigate settings where contemporary partial dependence methods – including FPD, Accumulated Local Effects (ALE), and SHapley Additive exPlanations (SHAP) methods – give biased results. We demonstrate that our approach works correctly on synthetic data and plausibly on real data sets. This work motivates a new line of inquiry into nonparametric partial dependence that provides robust information about the variables considered in a supervised learning task.;Machine Learning with Applications;10.1016/j.mlwa.2021.100146;https://www.sciencedirect.com/science/article/pii/S2666827021000736;15/12/2021;16/02/2022 10:00;16/02/2022 10:00;100146;2666-8270;HAJHKJDZ;journalArticle;;;6;Machine Learning with Applications;;;;
Journal paper;SCOPUS;1;search;1;2021;RFMap;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Mixte;Visual graphs;1;Python;Python;7;"Mazumdar, D.; Neto, M.P.; Paulovich, F.V.";Random forest similarity maps: A scalable visual representation for global and local interpretation;Machine Learning prediction algorithms have made significant contributions in today’s world, leading to increased usage in various domains. However, as ML algorithms surge, the need for transparent and interpretable models becomes essential. Visual representations have shown to be instrumental in addressing such an issue, allowing users to grasp models’ inner workings. Despite their popularity, visualization techniques still present visual scalability limitations, mainly when applied to analyze popular and complex models, such as Random Forests (RF). In this work, we propose Random Forest Similarity Map (RFMap), a scalable interactive visual analytics tool designed to analyze RF ensemble models. RFMap focuses on explaining the inner working mechanism of models through different views describing individual data instance predictions, providing an overview of the entire forest of trees, and highlighting instance input feature values. The interactive nature of RFMap allows users to visually interpret model errors and decisions, establishing the necessary confidence and user trust in RF models and improving performance. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.;Electronics (Switzerland);10.3390/electronics10222862;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119378667&doi=10.3390%2felectronics10222862&partnerID=40&md5=d46eab12152dd5879e05277597f35d79;2021;17/02/2022 13:39;17/02/2022 13:39;;;X78EYAQK;journalArticle;;22;10;;Scopus;Publisher: MDPI;<p>Export Date: 17 February 2022</p>;
Journal paper;SCOPUS;1;search;1;2021;VERONICA;Global;;model-specific;post hoc;Post-hoc;Template/Web interface;T./W.I.;Global overview;G.O.;Binaryclass;B.C;;Mixte;Visual graphs;2->1;R/Java;R;4;"Rostamzadeh, N.; Abdullah, S.S.; Sedig, K.; Garg, A.X.; McArthur, E.";Veronica: Visual analytics for identifying feature groups in disease classification;The use of data analysis techniques in electronic health records (EHRs) offers great promise in improving predictive risk modeling. Although useful, these analysis techniques often suffer from a lack of interpretability and transparency, especially when the data is high-dimensional. The emer-gence of a type of computational system known as visual analytics has the potential to address these issues by integrating data analysis techniques with interactive visualizations. This paper introduces a visual analytics system called VERONICA that utilizes the natural classification of features in EHRs to identify the group of features with the strongest predictive power. VERONICA incorporates a representative set of supervised machine learning techniques—namely, classification and regression tree, C5.0, random forest, support vector machines, and naive Bayes to support users in developing predictive models using EHRs. It then makes the analytics results accessible through an interactive visual interface. By integrating different sampling strategies, analytics algorithms, visualization techniques, and human-data interaction, VERONICA assists users in comparing prediction models in a systematic way. To demonstrate the usefulness and utility of our proposed system, we use the clinical dataset stored at ICES to identify the best representative feature groups in detecting patients who are at high risk of developing acute kidney injury. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.;Information (Switzerland);10.3390/info12090344;https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114199725&doi=10.3390%2finfo12090344&partnerID=40&md5=de8a088025077a77d57b71f4861ffafe;2021;17/02/2022 13:42;17/02/2022 13:42;;;2KMUK577;journalArticle;;9;12;;Scopus;Publisher: MDPI;"<p>Cited By :2</p>; <p>Export Date: 17 February 2022</p>";"Classification (of information); Classification and regression tree; Data analysis techniques; Data integration; Data visualization; Decision trees; Disease classification; Electronic health record (EHRs); Health risks; Interactive visualizations; Learning systems; Predictive analytics; Risk assessment; Supervised machine learning; Support vector machines; Support vector regression; Visual analytics systems; Visualization; Visualization technique"
Journal paper;Crossref;1;added;1;2017;ForEx;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Mixte;Text/statistics;;Not mentioned;Not mentioned;24;"Md Nasim Adnan; Md Zahidul Islam";ForEx++: A New Framework for Knowledge Discovery from Decision Forests;;Australasian Journal of Information Systems;10.3127/ajis.v21i0.1539;;;;;;;;;;;;Australasian Journal of Information Systems;Crossref;;;
Report;Report (newsletter (R news) 2002 published in semantic scholar in 2007);;added;;2007;VIP+PDP+PM;Hybrid;;model-specific;ant hoc;Intrinsic to the model ;Features oriented + Sample similarity-based;F.O. + S.S.;Global overview + Pattern discovery;G.O. + P.D.;All;Hybrid;;Mixte;Visual graphs;;R;R;18973;Andy Liaw and Matthew Wiener;Classification and Regression by randomForest;;;;;;;;;;;;;;;;;;;
Report;Report;;added;;2012;irfplot;Hybrid;;model-specific;post hoc;Post-hoc;Template/Web interface;T./W.I.;Global overview + Pattern discovery;G.O. + P.D.;All;Hybrid;;Mixte;Visual graphs;;R;R;12;Anna T. Quach ;Interactive Random Forests Plots;;;;;;;;;;;;;;;;;;;
Preprint;ArXiv;;added;;2015;ggRandomForests;Hybrid;;model-specific;post hoc;Post-hoc;Template/Web interface;T./W.I.;Global overview + Pattern discovery;G.O. + P.D.;Regression;Reg.;;Mixte;Visual graphs;;R;R;11;Ehrlinger, John;ggRandomForests: Visually Exploring a Random Forest for Regression;;ArXiv;;;;;;;;;;;;;;;;;
Report;Report;;added;;2016;edarf;Hybrid;;model-specific;post hoc;Post-hoc;Template/Web interface;T./W.I.;Global overview + Pattern discovery;G.O. + P.D.;All;Hybrid;;Mixte;Visual graphs;;R;R;44;Jones, Zachary M.;edarf:  Exploratory Data Analysis using Random Forests;;Journal of Open Source Software;;;;;;;;;;;;;;;;;
Report;Report;;added;;2018;Rfviz;Hybrid;;model-specific;post hoc;Post-hoc;Template/Web interface;T./W.I.;Global overview + Pattern discovery;G.O. + P.D.;All;Hybrid;;Mixte;Visual graphs;;R;R;1;Christopher Beckett ;Rfviz: An Interactive Visualization Package for Random Forests;;;;;;;;;;;;;;;;;;;
Report;Report;;added;;2017;randomForestExplainer;Global;;model-specific;post hoc;Post-hoc;Template/Web interface;T./W.I.;Global overview;G.O.;All;Hybrid;;Mixte;Visual graphs;;R;R;8;Aleksandra Paluszynska;Structure mining and knowledge extraction from random forest with applications to The Cancer Genome Atlas project;;;;;;;;;;;;;;;;;;;
Preprint;ArXiv;;added;;2020;SG and SM;Global;;model-specific;post hoc;Post-hoc;Sample similarity-based;S.S;Pattern discovery;P.D.;Multiclass;M.C.;;Mixte;Visual graphs;;Python;Python;56;Tan, Sarah;Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable;;ArXiv;arxiv.org/abs/1611.07115;;;;;;;;;;;;;;;;
Journal paper;Springer;;added;;2008;RISM;Hybrid;;model-specific;post hoc;Post-hoc;Size reduction;S.R.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Mixte;Text/statistics;;ACE system;Others;80;Anneleen Van Assche & Hendrik Blockeel ;Seeing the Forest Through the Trees;;Inductive Logic Programming;;;;;;;;;;;;;;;;;
Journal paper;Crossref;;added;;2009;sub-forests;Global;;model-specific;post hoc;Post-hoc;Size reduction;S.R.;Global overview;G.O.;All;Hybrid;;Mixte;Mixte;;Not mentioned;Not mentioned;79;Heping Zhang and Minghui Wang;Search for the smallest random forest;;Statistics and Its Interface;;;;;;;;;;;;;;;;;
Preprint;ArXiv;;added;;2016;forestFloor;Global;Local decomposition;model-specific;post hoc;Post-hoc;Features oriented;F.O.;Global overview;G.O.;All;Hybrid;;Mixte;Visual graphs;;R;R;73;Soeren H. Welling1,2, Hanne H.F. Refsgaard2, Per B. Brockhoff1 and Line H.Clemmensen;Forest Floor Visualizations of Random Forests;;ArXiv;;;;;;;;;;;;;;;;;
Preprint;ArXiv;;added;;2018;LORE;Local;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Local explanation;L.E.;Binaryclass;B.C;;Mixte;Text/statistics;;Python;Python;268;Riccardo Guidotti et al.;Local Rule-Based Explanations of Black Box Decision Systems;;ArXiv;;;;;;;;;;;;;;;;;
Journal paper;Crossref;;added;;2017;RF+DHC/SGL/MSGL;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;All;Hybrid;;Mixte;Text/statistics;;Not mentioned;Not mentioned;35;Morteza Mashayekhi* and Robin Gras;Rule Extraction from Decision Trees Ensembles: New Algorithms Based on Heuristic Search and Sparse Group Lasso Methods;;;;;;;;;;;;;;;;;;;
Preprint;ArXiv;;added;;2017;hara17;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;All;Hybrid;;Continuous;Text/statistics;;Python;Python;84;Satoshi Hara and Kohei Hayashi;Making Tree Ensembles Interpretable;;ArXiv;;;;;;;;;;;;;;;;;
Preprint;ArXiv;;added;;2021;OptExplain;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Pattern discovery + Local explanation;G.O. + P.D. + L.E.;Multiclass;M.C.;;Mixte;Text/statistics;;Python;Python;3;Gelin Zhang et al.;Extracting Optimal Explanations for Ensemble Trees via Logical Reasoning;;ArXiv;;;;;;;;;;;;;;;;;
Preprint;ArXiv;;added;;2008;RuleFit;Hybrid;;model-specific;post hoc;Post-hoc;Rule Extraction;R.E.;Global overview + Local explanation;G.O. + L.E.;Regression;Reg.;canbe used for binaryclassification;Mixte;Text/statistics;;R;R;938;Jerome H. Friedman and Bogdan E. Popescu;Predictive learning via rule ensembles;;Ann. Appl. Stat.;;;;;;;;;;;;;;;;;
